{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import time\nimport torch\nimport numpy as np\nimport json\nimport os\nfrom typing import Dict, List\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom datasets import load_dataset\nfrom tqdm import tqdm\n\n# Loading the model from a local directory\n# Load the Qwen/Qwen2.5-1.5B-Instruct model and tokenizer\nbase_dir = \"/kaggle/working/\"\nmodel_path = base_dir + \"models/Qwen2.5-1.5B-Instruct\"  # Path to your local model directory\nmodel_name = base_dir + model_path.split('/')[-1]\neval_dir = base_dir + f\"{model_name}-eval\"\nos.makedirs(eval_dir, exist_ok=True)\n\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_path,\n    torch_dtype=torch.float16,  # adjust based on your hardware\n    device_map=\"auto\"\n)\nmodel.eval()","metadata":{"_uuid":"1db48c48-44ee-4cb2-9a7e-abbcad6a46dc","_cell_guid":"8c56da68-0c7e-4a10-8ce1-5f53aa1acc7c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-02-13T19:29:56.094166Z","iopub.execute_input":"2025-02-13T19:29:56.094509Z","iopub.status.idle":"2025-02-13T19:29:56.101611Z","shell.execute_reply.started":"2025-02-13T19:29:56.094473Z","shell.execute_reply":"2025-02-13T19:29:56.100530Z"}},"outputs":[{"traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-faaca2668e67>\"\u001b[0;36m, line \u001b[0;32m13\u001b[0m\n\u001b[0;31m    base_dir =\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"],"ename":"SyntaxError","evalue":"invalid syntax (<ipython-input-1-faaca2668e67>, line 13)","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"def construct_prompt(example):\n    \"\"\"\n    Constructs a prompt from a dataset example.\n    Handles cases where 'choices' key is not present, using 'options' instead.\n    If neither is present, tries to use 'possible_answers'.\n    **Also handles the case where the key is 'answers' instead of 'choices', 'options', or 'possible_answers'.**\n    Assumes the following keys in example:\n      - \"question\": the question text.\n      - \"choices\" or \"options\" or \"possible_answers\" **or \"answers\"**: a list of answer choices.\n      - \"answer\": the correct answer letter (e.g., \"A\").\n    \"\"\"\n    # Check for all possible keys\n    choices_key = None\n    for key in [\"choices\", \"options\", \"possible_answers\", \"answers\"]:\n        if key in example:\n            choices_key = key\n            break\n\n    # Raise KeyError if none of the expected keys are found\n    if choices_key is None:\n        raise KeyError(\"Example must contain 'choices', 'options', 'possible_answers', or 'answers' key.\")\n\n    # Create the prompt with the question and enumerated choices.\n    prompt = f\"Question: {example['question']}\\nChoices:\\n\"\n    for idx, choice in enumerate(example[choices_key]):\n        # Convert index 0,1,2... to A, B, C, etc.\n        letter = chr(65 + idx)\n        prompt += f\"{letter}. {choice}\\n\"\n    prompt += \"Answer:\"  # model completion should output the answer letter.\n    return prompt\n\ndef evaluate_dataset(dataset, model, tokenizer):\n    \"\"\"\n    Iterates over the dataset, generates the model output for each prompt,\n    extracts the predicted answer (first capital letter found), and compares\n    it to the ground truth to compute accuracy.\n    \"\"\"\n    correct = 0\n    total = len(dataset)\n    for example in tqdm(dataset, desc=\"Evaluating\"):\n        prompt = construct_prompt(example)\n        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n        # Generate a short output; use deterministic generation for evaluation.\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=10,\n            temperature=0.0,\n            do_sample=False\n        )\n        # Decode the generated tokens.\n        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        # Extract the text generated after the prompt.\n        generated_text = response[len(prompt):].strip()\n        # Find the first capital letter (A, B, C, …) as predicted answer.\n        answer_pred = \"\"\n        for char in generated_text:\n            if char.upper() in [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\"]:\n                answer_pred = char.upper()\n                break\n        # Compare with the ground truth answer.\n        # Convert the ground truth answer to a letter if it's an integer.\n        ground_truth_answer = example[\"answer\"]\n        if isinstance(ground_truth_answer, int):\n            ground_truth_answer = chr(65 + ground_truth_answer)  # Convert 0 to A, 1 to B, etc.\n        else:\n            ground_truth_answer = ground_truth_answer.strip().upper()\n\n        if answer_pred == ground_truth_answer:\n            correct += 1\n    accuracy = correct / total * 100\n    return accuracy","metadata":{"_uuid":"1bc06135-ae3d-451a-ad29-0888d3b32eb4","_cell_guid":"3b795294-10d8-4fc7-855d-2adc4f9ffe7c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# English Evaluation (cais/mmlu)\n# List of all MMLU tasks\nmmlu_tasks = [\n    'abstract_algebra', 'anatomy', 'astronomy', 'auxiliary_train',\n    'business_ethics', 'clinical_knowledge', 'college_biology', 'college_chemistry',\n    'college_computer_science', 'college_mathematics', 'college_medicine', 'college_physics',\n    'computer_security', 'conceptual_physics', 'econometrics', 'electrical_engineering',\n    'elementary_mathematics', 'formal_logic', 'global_facts', 'high_school_biology',\n    'high_school_chemistry', 'high_school_computer_science', 'high_school_european_history',\n    'high_school_geography', 'high_school_government_and_politics', 'high_school_macroeconomics',\n    'high_school_mathematics', 'high_school_microeconomics', 'high_school_physics',\n    'high_school_psychology', 'high_school_statistics', 'high_school_us_history',\n    'high_school_world_history', 'human_aging', 'human_sexuality', 'international_law',\n    'jurisprudence', 'logical_fallacies', 'machine_learning', 'management', 'marketing',\n    'medical_genetics', 'miscellaneous', 'moral_disputes', 'moral_scenarios', 'nutrition',\n    'philosophy', 'prehistory', 'professional_accounting', 'professional_law',\n    'professional_medicine', 'professional_psychology', 'public_relations', 'security_studies',\n    'sociology', 'us_foreign_policy', 'virology', 'world_religions'\n]\n\n# Evaluate the model on all MMLU tasks\nmmlu_results = {'english': {}, 'russian': {}}\n\n# English evaluation\nfor task in mmlu_tasks:\n    try:\n        dataset_mmlu = load_dataset(\"cais/mmlu\", task, split=\"test\")\n        print(f\"Evaluating on cais/mmlu dataset ({task})\")\n        accuracy_mmlu = evaluate_dataset(dataset_mmlu, model, tokenizer)\n        print(f\"Accuracy on cais/mmlu ({task}): {accuracy_mmlu:.2f}%\")\n        mmlu_results['english'][task] = accuracy_mmlu\n    except Exception as e:\n        print(f\"Error evaluating {task}: {str(e)}\")\n\nmmlu_results['english'][\"average\"] = sum(mmlu_results['english'].values()) / len(mmlu_results['english'])\n\n# Russian evaluation\nfor task in mmlu_tasks:\n    try:\n        dataset_mmlu = load_dataset(\"cais/mmlu\", task, split=\"test\")\n        print(f\"Evaluating on cais/mmlu dataset ({task})\")\n        accuracy_mmlu = evaluate_dataset(dataset_mmlu, model, tokenizer)\n        print(f\"Accuracy on cais/mmlu ({task}): {accuracy_mmlu:.2f}%\")\n        mmlu_results['russian'][task] = accuracy_mmlu\n    except Exception as e:\n        print(f\"Error evaluating {task}: {str(e)}\")\n\nmmlu_results['russian'][\"average\"] = sum(mmlu_results['russian'].values()) / len(mmlu_results['russian'])\n\n# Save MMLU results\nwith open(f\"{eval_dir}/mmlu.json\", 'w') as f:\n    json.dump(mmlu_results, f, indent=2)","metadata":{"_uuid":"f5d17feb-3279-4b82-a866-2b06c7e9dee3","_cell_guid":"0d54e245-77bb-4212-9e5d-011bfa19831f","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def measure_generation_performance(\n    model,\n    tokenizer,\n    text: str,\n    n_runs: int = 5\n) -> Dict[str, float]:\n    \"\"\"Measure generation performance metrics for a given text.\"\"\"\n    metrics = {\n        'token_count': [],\n        'generation_time': [],\n        'tokens_per_second': []\n    }\n\n    # Tokenize once to get input token count\n    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n    input_token_count = inputs.input_ids.size(1)\n\n    # Run multiple times to get stable measurements\n    for _ in range(n_runs):\n        start_time = time.time()\n        with torch.no_grad():\n            outputs = model.generate(\n                **inputs,\n                max_new_tokens=50,  # Adjust based on your needs\n                temperature=0.7\n            )\n        generation_time = time.time() - start_time\n\n        output_token_count = outputs.size(1) - input_token_count\n        tokens_per_second = output_token_count / generation_time if generation_time > 0 else 0\n\n        metrics['token_count'].append(output_token_count)\n        metrics['generation_time'].append(generation_time)\n        metrics['tokens_per_second'].append(tokens_per_second)\n\n    return {\n        'input_tokens': input_token_count,\n        'avg_output_tokens': np.mean(metrics['token_count']),\n        'avg_generation_time': np.mean(metrics['generation_time']),\n        'avg_tokens_per_second': np.mean(metrics['tokens_per_second']),\n        'std_tokens_per_second': np.std(metrics['tokens_per_second'])\n    }\n\ndef compare_language_performance(model, tokenizer):\n    \"\"\"Compare performance between English and Russian text generation.\"\"\"\n\n    # Sample texts (roughly equivalent content)\n    texts = {\n        'english': \"\"\"\n        Please write a short story about a cat who discovers a magical garden.\n        The story should be appropriate for children and include some description\n        of the flowers and plants the cat encounters.\n        \"\"\",\n\n        'russian': \"\"\"\n        Пожалуйста, напишите короткий рассказ о коте, который обнаруживает волшебный сад.\n        Рассказ должен быть подходящим для детей и включать описание\n        цветов и растений, которые встречает кот.\n        \"\"\"\n    }\n\n    # Measure performance for each language\n    speed_results = {}\n    for lang, text in texts.items():\n        print(f\"\\nMeasuring {lang} performance...\")\n        speed_results[lang] = measure_generation_performance(model, tokenizer, text)\n\n    # Print comparative results\n    print(\"\\n=== Performance Comparison ===\")\n    metrics = ['input_tokens', 'avg_output_tokens', 'avg_generation_time',\n               'avg_tokens_per_second']\n\n    for metric in metrics:\n        print(f\"\\n{metric}:\")\n        for lang in speed_results:\n            print(f\"{lang}: {speed_results[lang][metric]:.2f}\")\n\n        # Calculate relative difference\n        if 'english' in speed_results and 'russian' in speed_results:\n            diff_percent = ((speed_results['russian'][metric] - speed_results['english'][metric])\n                          / speed_results['english'][metric] * 100)\n            print(f\"Relative difference: {diff_percent:+.1f}%\")\n\n    # Save speed results\n    with open(f\"{eval_dir}/speed.json\", 'w') as f:\n        json.dump(speed_results, f, indent=2)\n\n    return speed_results","metadata":{"_uuid":"6ac256d7-e22f-4fdd-8ad4-797bc858fad8","_cell_guid":"8927a62f-1210-4c31-a81a-ad5a6d53b282","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"results = compare_language_performance(model, tokenizer)","metadata":{"_uuid":"d2b232a4-9b14-4be5-8f66-c6004569d7e1","_cell_guid":"aa0979b6-3027-4a74-aae9-954a260cb998","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}