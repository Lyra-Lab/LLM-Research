{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10744015,"sourceType":"datasetVersion","datasetId":6662849}],"dockerImageVersionId":30886,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom datasets import load_dataset\nfrom tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T14:03:03.581167Z","iopub.execute_input":"2025-02-15T14:03:03.581498Z","iopub.status.idle":"2025-02-15T14:03:14.678586Z","shell.execute_reply.started":"2025-02-15T14:03:03.581472Z","shell.execute_reply":"2025-02-15T14:03:14.677749Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"def construct_prompt(example, lang=\"en\"):\n    \"\"\"\n    Constructs a prompt from a dataset example.\n    Uses language-specific keys if available and falls back to generic keys.\n    \"\"\"\n    if lang.lower() == \"ru\":\n        question_key = \"question_ru\"\n        choices_key = \"choices_ru\"\n    else:\n        question_key = \"question_en\"\n        choices_key = \"choices_en\"\n        \n    if choices_key not in example:\n        for key in [\"choices\", \"options\", \"possible_answers\", \"answers\"]:\n            if key in example:\n                choices_key = key\n                break\n    if choices_key not in example:\n        raise KeyError(\"Example must contain a valid choices key.\")\n        \n    question_text = example.get(question_key, example.get(\"question\", \"\"))\n    prompt = f\"Question: {question_text}\\nChoices:\\n\"\n    for idx, choice in enumerate(example[choices_key]):\n        letter = chr(65 + idx)  # Map 0->A, 1->B, etc.\n        prompt += f\"{letter}. {choice}\\n\"\n    prompt += \"Answer:\"\n    return prompt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T14:03:19.832636Z","iopub.execute_input":"2025-02-15T14:03:19.832926Z","iopub.status.idle":"2025-02-15T14:03:19.838065Z","shell.execute_reply.started":"2025-02-15T14:03:19.832903Z","shell.execute_reply":"2025-02-15T14:03:19.837255Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def evaluate_dataset(dataset, model, tokenizer, device):\n    \"\"\"\n    Iterates over the dataset, generates the model output for each prompt,\n    extracts the predicted answer (first capital letter found), and compares\n    it to the ground truth to compute accuracy.\n    \"\"\"\n    correct = 0\n    total = len(dataset)\n    for example in tqdm(dataset, desc=\"Evaluating\"):\n        prompt = construct_prompt(example, lang=\"ru\")\n        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=10,\n            temperature=0.0,\n            do_sample=False\n        )\n        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        generated_text = response[len(prompt):].strip()\n        answer_pred = \"\"\n        for char in generated_text:\n            if char.upper() in [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\"]:\n                answer_pred = char.upper()\n                break\n        ground_truth = example[\"answer\"]\n        if isinstance(ground_truth, int):\n            ground_truth = chr(65 + ground_truth)\n        else:\n            ground_truth = ground_truth.strip().upper()\n        if answer_pred == ground_truth:\n            correct += 1\n    accuracy = correct / total * 100\n    return accuracy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T14:03:22.561876Z","iopub.execute_input":"2025-02-15T14:03:22.562152Z","iopub.status.idle":"2025-02-15T14:03:22.568117Z","shell.execute_reply.started":"2025-02-15T14:03:22.562132Z","shell.execute_reply":"2025-02-15T14:03:22.567431Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def main():\n    # Set device based on availability. With device_map=\"auto\", the model will be distributed\n    # across available GPUs (e.g. 2 T4 GPUs in Kaggle).\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    \n    model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        torch_dtype=torch.float16,  # use fp16 for efficiency\n        device_map=\"auto\"           # automatically uses available GPUs\n    )\n    model.eval()\n    \n    # List of all MMLU (Russian) tasks to evaluate on\n    mmlu_ru_tasks = [\n      'abstract_algebra', 'anatomy', 'astronomy', 'auxiliary_train',\n      'business_ethics', 'clinical_knowledge', 'college_biology', 'college_chemistry',\n      'college_computer_science', 'college_mathematics', 'college_medicine', 'college_physics',\n      'computer_security', 'conceptual_physics', 'econometrics', 'electrical_engineering',\n      'elementary_mathematics', 'formal_logic', 'global_facts', 'high_school_biology',\n      'high_school_chemistry', 'high_school_computer_science', 'high_school_european_history',\n      'high_school_geography', 'high_school_government_and_politics', 'high_school_macroeconomics',\n      'high_school_mathematics', 'high_school_microeconomics', 'high_school_physics',\n      'high_school_psychology', 'high_school_statistics', 'high_school_us_history',\n      'high_school_world_history', 'human_aging', 'human_sexuality', 'international_law',\n      'jurisprudence', 'logical_fallacies', 'machine_learning', 'management', 'marketing',\n      'medical_genetics', 'miscellaneous', 'moral_disputes', 'moral_scenarios', 'nutrition',\n      'philosophy', 'prehistory', 'professional_accounting', 'professional_law',\n      'professional_medicine', 'professional_psychology', 'public_relations', 'security_studies',\n      'sociology', 'us_foreign_policy', 'virology', 'world_religions'\n    ]\n    \n    results = {}\n    for task in mmlu_ru_tasks:\n        try:\n            dataset_mmlu = load_dataset(\"NLPCoreTeam/mmlu_ru\", task, split=\"test\")\n            print(f\"Evaluating on NLPCoreTeam/mmlu_ru dataset ({task})\")\n            accuracy = evaluate_dataset(dataset_mmlu, model, tokenizer, device)\n            print(f\"Accuracy on {task}: {accuracy:.2f}%\")\n            results[task] = accuracy\n        except Exception as e:\n            print(f\"Error evaluating {task}: {e}\")\n    \n    print(\"\\nResults:\")\n    for task, acc in results.items():\n        print(f\"{task}: {acc:.2f}%\")\n    if results:\n        avg_accuracy = sum(results.values()) / len(results)\n        print(f\"\\nAverage accuracy: {avg_accuracy:.2f}%\")\n    else:\n        print(\"No evaluation results.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T14:03:41.587993Z","iopub.execute_input":"2025-02-15T14:03:41.588293Z","iopub.status.idle":"2025-02-15T14:03:41.594918Z","shell.execute_reply.started":"2025-02-15T14:03:41.588267Z","shell.execute_reply":"2025-02-15T14:03:41.594034Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T14:03:45.110967Z","iopub.execute_input":"2025-02-15T14:03:45.111322Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/7.30k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0ad5cbf9edf4f88aaaae5332d5330de"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a53348aaadc496586e258f088517b9a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ad587e16c8b41c59bcd5bc332e86502"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e5eb5ac48784138b8138649de23e3ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/663 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c7f27d70da94d1292c8c6e146372d6c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/27.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9edc8e75634b4af1bf2cc3a72e033583"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"418b5bc200194922ba0e700a997be55f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/3.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"884bf94922364ed0b546942b91da1d9a"}},"metadata":{}}],"execution_count":null}]}