{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10744015,"sourceType":"datasetVersion","datasetId":6662849}],"dockerImageVersionId":30886,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom datasets import load_dataset\nfrom datetime import datetime\nfrom typing import Dict, List, Optional, Union\nfrom tqdm import tqdm\nimport pandas as pd\nimport os\nimport time","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T09:29:16.582036Z","iopub.execute_input":"2025-02-16T09:29:16.582379Z","iopub.status.idle":"2025-02-16T09:29:16.587281Z","shell.execute_reply.started":"2025-02-16T09:29:16.582355Z","shell.execute_reply":"2025-02-16T09:29:16.586093Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"# Evaluate base model on MMLU VS MMLU_RU performance","metadata":{}},{"cell_type":"markdown","source":"## This class is used to properly manager performance results","metadata":{}},{"cell_type":"code","source":"class ModelPerformanceTracker:\n    \"\"\"\n    Handles tracking and managing model performance metrics for various evaluations.\n    \"\"\"\n    def __init__(self, dataset_path=\"model_performance_metrics\"):\n        self.dataset_path = dataset_path\n        self.metrics_file = os.path.join(dataset_path, \"metrics.csv\")\n        self.ensure_dataset_structure()\n        \n    def ensure_dataset_structure(self):\n        \"\"\"Creates necessary directories and files if they don't exist.\"\"\"\n        os.makedirs(self.dataset_path, exist_ok=True)\n        if not os.path.exists(self.metrics_file):\n            df = pd.DataFrame(columns=[\n                'model_name', 'metric_type', 'metric_name', 'value',\n                'task', 'language', 'model_params', 'timestamp', \n                'run_id', 'training_config', 'notes'\n            ])\n            df.to_csv(self.metrics_file, index=False)\n\n    def log_metrics(self, \n                   metrics: Dict[str, Union[float, Dict[str, float]]], \n                   model_name: str,\n                   metric_type: str,\n                   task: Optional[str] = None,\n                   language: Optional[str] = None,\n                   model_params: Optional[dict] = None,\n                   training_config: Optional[dict] = None,\n                   notes: Optional[str] = None) -> str:\n        \"\"\"\n        Logs various model metrics to the dataset.\n        \n        Args:\n            metrics: Dictionary of metric_name:value pairs or metric_name:dict pairs\n            model_name: Name of the model being evaluated\n            metric_type: Type of metric (e.g., 'accuracy', 'speed', 'memory')\n            task: Task name if applicable\n            language: Language of evaluation if applicable\n            model_params: Model parameters/configuration\n            training_config: Training configuration if model was fine-tuned\n            notes: Additional notes about the evaluation\n        \n        Returns:\n            run_id: Unique identifier for this evaluation run\n        \"\"\"\n        df = pd.read_csv(self.metrics_file)\n        run_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        \n        new_entries = []\n        for metric_name, value in metrics.items():\n            if isinstance(value, dict):\n                # Handle nested metrics (e.g., per-task accuracies)\n                for sub_name, sub_value in value.items():\n                    entry = {\n                        'model_name': model_name,\n                        'metric_type': metric_type,\n                        'metric_name': f\"{metric_name}/{sub_name}\",\n                        'value': sub_value,\n                        'task': task,\n                        'language': language,\n                        'model_params': str(model_params) if model_params else None,\n                        'timestamp': datetime.now().isoformat(),\n                        'run_id': run_id,\n                        'training_config': str(training_config) if training_config else None,\n                        'notes': notes\n                    }\n                    new_entries.append(entry)\n            else:\n                entry = {\n                    'model_name': model_name,\n                    'metric_type': metric_type,\n                    'metric_name': metric_name,\n                    'value': value,\n                    'task': task,\n                    'language': language,\n                    'model_params': str(model_params) if model_params else None,\n                    'timestamp': datetime.now().isoformat(),\n                    'run_id': run_id,\n                    'training_config': str(training_config) if training_config else None,\n                    'notes': notes\n                }\n                new_entries.append(entry)\n        \n        df_new = pd.DataFrame(new_entries)\n        df = pd.concat([df, df_new], ignore_index=True)\n        df.to_csv(self.metrics_file, index=False)\n        \n        return run_id\n\n    def get_metrics(self, \n                   model_name: Optional[str] = None,\n                   metric_type: Optional[str] = None,\n                   task: Optional[str] = None,\n                   run_id: Optional[str] = None) -> pd.DataFrame:\n        \"\"\"\n        Retrieves metrics based on specified filters.\n        \"\"\"\n        df = pd.read_csv(self.metrics_file)\n        \n        if model_name:\n            df = df[df['model_name'] == model_name]\n        if metric_type:\n            df = df[df['metric_type'] == metric_type]\n        if task:\n            df = df[df['task'] == task]\n        if run_id:\n            df = df[df['run_id'] == run_id]\n            \n        return df\n\n    def compare_models(self, \n                      model_names: List[str],\n                      metric_type: Optional[str] = None) -> pd.DataFrame:\n        \"\"\"\n        Compares metrics between different models.\n        \"\"\"\n        df = pd.read_csv(self.metrics_file)\n        df = df[df['model_name'].isin(model_names)]\n        \n        if metric_type:\n            df = df[df['metric_type'] == metric_type]\n            \n        comparison = df.groupby(['model_name', 'metric_type', 'metric_name']).agg({\n            'value': ['mean', 'std', 'count'],\n            'timestamp': 'max'\n        }).round(4)\n        \n        return comparison","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-02-16T08:37:23.968514Z","iopub.execute_input":"2025-02-16T08:37:23.968874Z","iopub.status.idle":"2025-02-16T08:37:23.982351Z","shell.execute_reply.started":"2025-02-16T08:37:23.968822Z","shell.execute_reply":"2025-02-16T08:37:23.981348Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## These are the main evaluation function that are used to evaluate the model on any task in mmlu","metadata":{}},{"cell_type":"code","source":"def construct_prompt(example, lang=\"en\"):\n    \"\"\"\n    Constructs a prompt from a dataset example.\n    Uses language-specific keys if available and falls back to generic keys.\n    \"\"\"\n    if lang.lower() == \"ru\":\n        question_key = \"question_ru\"\n        choices_key = \"choices_ru\"\n    else:\n        question_key = \"question_en\"\n        choices_key = \"choices_en\"\n        \n    if choices_key not in example:\n        for key in [\"choices\", \"options\", \"possible_answers\", \"answers\"]:\n            if key in example:\n                choices_key = key\n                break\n    if choices_key not in example:\n        raise KeyError(\"Example must contain a valid choices key.\")\n        \n    question_text = example.get(question_key, example.get(\"question\", \"\"))\n    prompt = f\"Question: {question_text}\\nChoices:\\n\"\n    for idx, choice in enumerate(example[choices_key]):\n        letter = chr(65 + idx)  # Map 0->A, 1->B, etc.\n        prompt += f\"{letter}. {choice}\\n\"\n    prompt += \"Answer:\"\n    return prompt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T08:37:26.385828Z","iopub.execute_input":"2025-02-16T08:37:26.386167Z","iopub.status.idle":"2025-02-16T08:37:26.392393Z","shell.execute_reply.started":"2025-02-16T08:37:26.386144Z","shell.execute_reply":"2025-02-16T08:37:26.391329Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def evaluate_dataset(dataset, model, tokenizer, device):\n    \"\"\"\n    Iterates over the dataset, generates the model output for each prompt,\n    extracts the predicted answer (first capital letter found), and compares\n    it to the ground truth to compute accuracy.\n    \"\"\"\n    correct = 0\n    total = len(dataset)\n    for example in tqdm(dataset, desc=\"Evaluating\"):\n        prompt = construct_prompt(example, lang=\"ru\")\n        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=10,\n            temperature=0.0,\n            do_sample=False\n        )\n        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        generated_text = response[len(prompt):].strip()\n        answer_pred = \"\"\n        for char in generated_text:\n            if char.upper() in [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\"]:\n                answer_pred = char.upper()\n                break\n        ground_truth = example[\"answer\"]\n        if isinstance(ground_truth, int):\n            ground_truth = chr(65 + ground_truth)\n        else:\n            ground_truth = ground_truth.strip().upper()\n        if answer_pred == ground_truth:\n            correct += 1\n    accuracy = correct / total * 100\n    return accuracy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T08:37:29.544740Z","iopub.execute_input":"2025-02-16T08:37:29.545153Z","iopub.status.idle":"2025-02-16T08:37:29.552420Z","shell.execute_reply.started":"2025-02-16T08:37:29.545122Z","shell.execute_reply":"2025-02-16T08:37:29.551285Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## Initializing the model","metadata":{}},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nmodel_name = \"Qwen/Qwen2.5-7B-Instruct\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=torch.float16,\n    device_map=\"auto\"\n)\nmodel.eval()\n\ntracker = ModelPerformanceTracker()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T09:29:20.058513Z","iopub.execute_input":"2025-02-16T09:29:20.058902Z","iopub.status.idle":"2025-02-16T09:29:25.361445Z","shell.execute_reply.started":"2025-02-16T09:29:20.058836Z","shell.execute_reply":"2025-02-16T09:29:25.359837Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f869f5a6a47c428286f17662e04d95fa"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-ac604f768fd4>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Qwen/Qwen2.5-7B-Instruct\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m model = AutoModelForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mmodel_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 564\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    565\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4262\u001b[0m                     \u001b[0moffload_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4263\u001b[0m                     \u001b[0merror_msgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4264\u001b[0;31m                 \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_pretrained_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4265\u001b[0m                     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4266\u001b[0m                     \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules, gguf_path, weights_only)\u001b[0m\n\u001b[1;32m   4775\u001b[0m                                 )\n\u001b[1;32m   4776\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4777\u001b[0;31m                         new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(\n\u001b[0m\u001b[1;32m   4778\u001b[0m                             \u001b[0mmodel_to_load\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4779\u001b[0m                             \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, hf_quantizer, is_safetensors, keep_in_fp32_modules, unexpected_keys, pretrained_model_name_or_path)\u001b[0m\n\u001b[1;32m    884\u001b[0m                     \u001b[0mset_module_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 886\u001b[0;31m                 \u001b[0mparam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;31m# For compatibility with PyTorch load_state_dict which converts state dict dtype to existing dtype in model, and which\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":11},{"cell_type":"code","source":"mmlu_tasks = [\n    'abstract_algebra',                 # Advanced mathematics\n    'college_mathematics',              # Mathematics at college level\n    'college_physics',                  # Physics at college level\n    'college_chemistry',                # Chemistry at college level\n    'college_computer_science',         # Computer science fundamentals\n    'computer_security',                # Computer security\n    'machine_learning',                 # Machine learning (critical inclusion)\n    'clinical_knowledge',               # Medical and clinical reasoning\n    'college_medicine',                 # Medicine at college level\n    'international_law',                # Law and international legal standards\n    'business_ethics',                  # Ethics in business contexts\n    'econometrics',                     # Economics and statistical methods\n    'electrical_engineering',           # Engineering principles\n    'global_facts',                     # General world knowledge\n    'high_school_government_and_politics', # Social studies and politics\n    'high_school_mathematics',          # High school mathematics\n    'high_school_physics',              # High school physics\n    'philosophy',                       # Philosophical reasoning\n    'sociology',                        # Social sciences\n    'world_religions',                  # Comparative cultural/religious knowledge\n    'management',                       # Business management\n    'marketing',                        # Marketing principles\n    'logical_fallacies',                # Logical reasoning and critical thinking\n    'college_biology',                  # Life sciences at college level\n    'astronomy'                         # Astronomy and space science\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T09:29:29.740937Z","iopub.execute_input":"2025-02-16T09:29:29.741285Z","iopub.status.idle":"2025-02-16T09:29:29.746177Z","shell.execute_reply.started":"2025-02-16T09:29:29.741257Z","shell.execute_reply":"2025-02-16T09:29:29.745041Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"## Evaluation","metadata":{}},{"cell_type":"code","source":"# Evaluate MMLU_RU performance\nmmlu_results = {}\nfor task in mmlu_tasks:\n    try:\n        dataset_mmlu = load_dataset(\"NLPCoreTeam/mmlu_ru\", task, split=\"test\")\n        accuracy = evaluate_dataset(dataset_mmlu, model, tokenizer, device)\n        mmlu_results[task] = accuracy\n    except Exception as e:\n        print(f\"Error evaluating {task}: {e}\")\n\n# Log MMLU_RU results\nrun_id = tracker.log_metrics(\n    metrics={'task_accuracy': mmlu_results},\n    model_name=model_name,\n    metric_type='accuracy',\n    language=\"ru\",\n    model_params={\n        \"model_size\": \"7B\",\n        \"dtype\": \"float16\",\n        \"device\": str(device)\n    },\n    notes=\"MMLU-RU evaluation\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T09:29:31.395292Z","iopub.execute_input":"2025-02-16T09:29:31.395665Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/36.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"553fbde993a84272ba141e014a644e8d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"mmlu_ru.py:   0%|          | 0.00/5.68k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f2ac7c3f3364409acd214570af9ce8b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0000.parquet:   0%|          | 0.00/6.02k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c611a7c61c6842269d55037d5d138637"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0000.parquet:   0%|          | 0.00/6.33k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2735f2be237e4482833d190ee36b91ae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0000.parquet:   0%|          | 0.00/21.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"296ed1ca22914116b5c0c903b14391b7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating dev split:   0%|          | 0/5 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c976c3a3121847c8a5cf825bb6dcfb98"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating val split:   0%|          | 0/11 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41bb3c24bd1448b3b9bf4ad3dd5b7b00"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1fb09cc40a242e992fe34ac9c6d8700"}},"metadata":{}},{"name":"stderr","text":"Evaluating:   0%|          | 0/100 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:650: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n  warnings.warn(\nEvaluating: 100%|██████████| 100/100 [01:08<00:00,  1.46it/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0000.parquet:   0%|          | 0.00/10.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"370aae152223495a8031fc06136235ae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0000.parquet:   0%|          | 0.00/9.48k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8fcf9f54e87f41128c538bba25ddea1a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0000.parquet:   0%|          | 0.00/37.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a805fefe1235454e95b198753af0cbe1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating dev split:   0%|          | 0/5 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed39a78032c448b6800de507c7aa4643"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating val split:   0%|          | 0/11 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"622e0d9324df4981ad64d11d17ae38c7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f6985ffc0444ff39ef13f3eca4ae4bd"}},"metadata":{}},{"name":"stderr","text":"Evaluating: 100%|██████████| 100/100 [01:09<00:00,  1.44it/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0000.parquet:   0%|          | 0.00/8.15k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bac07039eaf749afa35a3020fb851c41"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0000.parquet:   0%|          | 0.00/12.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce8dd8c38d0746b1afcf9a0b8fc32662"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0000.parquet:   0%|          | 0.00/43.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e4100cda40a45ceb846cf32158d8638"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating dev split:   0%|          | 0/5 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1dec22c1d6c94f51aae69d4db9f91db3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating val split:   0%|          | 0/11 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09a4ab9ce662482cbf4167f4989f6980"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/102 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16b7aa7397784d5297a55d84874c105a"}},"metadata":{}},{"name":"stderr","text":"Evaluating: 100%|██████████| 102/102 [01:11<00:00,  1.42it/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0000.parquet:   0%|          | 0.00/7.47k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"117dd68536e54dc0b328698b208ee9ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0000.parquet:   0%|          | 0.00/9.41k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c079c1e88a61459b8c28725c48fb8e11"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0000.parquet:   0%|          | 0.00/40.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6491c4136ff94fdcb0233395634f51a7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating dev split:   0%|          | 0/5 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d4d0528e873405c9991ee2eab87cc99"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating val split:   0%|          | 0/8 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b29c7be26c6748569fb59d54b31695b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0d7f4cd5e36427781989e1f0aff97da"}},"metadata":{}},{"name":"stderr","text":"Evaluating: 100%|██████████| 100/100 [01:09<00:00,  1.43it/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0000.parquet:   0%|          | 0.00/12.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60de371251004d33b16064e369d638b6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0000.parquet:   0%|          | 0.00/14.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1206b94510b422889b64c738f8568bf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0000.parquet:   0%|          | 0.00/73.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91b1b68637644f0197946353c21797ae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating dev split:   0%|          | 0/5 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3093536058374e718091185f920482f5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating val split:   0%|          | 0/11 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aee3c7fd070846c0b92b4c086eaf61b4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb167c8e545f4a14867c3e56d1e4b16b"}},"metadata":{}},{"name":"stderr","text":"Evaluating: 100%|██████████| 100/100 [01:13<00:00,  1.37it/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0000.parquet:   0%|          | 0.00/6.96k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff178471e298413796ce67be6f10bd24"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0000.parquet:   0%|          | 0.00/13.4k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9bf558b97e2f48c5b14920dcd69d394d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0000.parquet:   0%|          | 0.00/44.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03295cc980e843b3ac8fe80178a542ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating dev split:   0%|          | 0/5 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd3f9631d74d41189a23538e4c11a7ff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating val split:   0%|          | 0/11 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8afce49a468e41b4b77c2cd121ce8ecc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a292f83fad6a4b8eae4e2c9af6469ee0"}},"metadata":{}},{"name":"stderr","text":"Evaluating: 100%|██████████| 100/100 [01:08<00:00,  1.45it/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0000.parquet:   0%|          | 0.00/10.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8aacff554312431caabd1709c9b784fe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0000.parquet:   0%|          | 0.00/11.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7d51f4e009c4a50b8fa1916768c1f44"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0000.parquet:   0%|          | 0.00/47.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"25cfb64c957943ce9ac003dacd4e43ed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating dev split:   0%|          | 0/5 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"695749fd5ba54f859e94c6cc64538b2c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating val split:   0%|          | 0/11 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f233b568658348f58a222e0f2d6828ad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/112 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64ee3704fbc04a6f90e628c7cec9a987"}},"metadata":{}},{"name":"stderr","text":"Evaluating: 100%|██████████| 112/112 [01:19<00:00,  1.41it/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0000.parquet:   0%|          | 0.00/6.16k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c629305d119e49858524f17c287b1fc0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0000.parquet:   0%|          | 0.00/15.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c47e12d0dee24325af855eee23696f46"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0000.parquet:   0%|          | 0.00/99.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"995aa076202c450ca0956a747d890d40"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating dev split:   0%|          | 0/5 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc5ede15b81a4dc2abe3c5c82f87221c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating val split:   0%|          | 0/29 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1723643ef39d4a07bac12b1508ea88f5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/265 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ebf53a9d382d485b87b9d6fedf447e60"}},"metadata":{}},{"name":"stderr","text":"Evaluating:  79%|███████▉  | 209/265 [02:23<00:38,  1.44it/s]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# Evaluate MMLU performance\nmmlu_results = {}\nfor task in mmlu_tasks:\n    try:\n        # Load the original MMLU dataset\n        dataset_mmlu = load_dataset(\"cais/mmlu\", task, split=\"test\")\n        # Evaluate the dataset\n        accuracy = evaluate_dataset(dataset_mmlu, model, tokenizer, device)\n        # Store the results\n        mmlu_results[task] = accuracy\n    except Exception as e:\n        print(f\"Error evaluating {task}: {e}\")\n\n# Log MMLU results\nrun_id = tracker.log_metrics(\n    metrics={'task_accuracy': mmlu_results},\n    model_name=model_name,\n    metric_type='accuracy',\n    language=\"en\",\n    model_params={\n        \"model_size\": \"7B\",\n        \"dtype\": \"float16\",\n        \"device\": str(device)\n    },\n    notes=\"MMLU evaluation\"\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Evaluate the speed of text generation in Russian VS English","metadata":{}},{"cell_type":"code","source":"def evaluate_generation_speed(model, tokenizer, device, \n                            num_samples=100, \n                            input_lengths=[128, 512], \n                            output_lengths=[128, 512]):\n    \"\"\"\n    Evaluates model generation speed for different input and output lengths.\n    \n    Returns:\n        Dictionary containing various speed metrics:\n        - tokens_per_second: Generation speed\n        - latency_ms: Average latency per generation\n        - throughput: Tokens processed per second including input\n    \"\"\"\n    metrics = {}\n    \n    # Generate sample input text of different lengths\n    sample_text = \"The quick brown fox jumps over the lazy dog. \" * 100\n    \n    for input_len in input_lengths:\n        for output_len in output_lengths:\n            total_generation_time = 0\n            total_tokens = 0\n            \n            # Truncate input text to desired length\n            input_text = sample_text[:input_len]\n            \n            # Warm-up run\n            inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n            _ = model.generate(**inputs, max_new_tokens=output_len)\n            \n            # Timed runs\n            for _ in range(num_samples):\n                inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n                start_time = time.time()\n                outputs = model.generate(**inputs, max_new_tokens=output_len)\n                end_time = time.time()\n                \n                generation_time = end_time - start_time\n                total_generation_time += generation_time\n                total_tokens += len(outputs[0]) - len(inputs['input_ids'][0])\n            \n            # Calculate metrics\n            avg_latency = (total_generation_time / num_samples) * 1000  # ms\n            tokens_per_second = total_tokens / total_generation_time\n            \n            metrics[f\"input_{input_len}_output_{output_len}\"] = {\n                'latency_ms': avg_latency,\n                'tokens_per_second': tokens_per_second,\n            }\n    \n    return metrics","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Evaluate generation speed\nspeed_metrics = evaluate_generation_speed(\n    model, \n    tokenizer, \n    device,\n    num_samples=10  # Reduced for testing, increase for better statistics\n)\n\n# Log speed metrics\ntracker.log_metrics(\n    metrics=speed_metrics,\n    model_name=model_name,\n    metric_type='speed',\n    model_params={\n        \"model_size\": \"7B\",\n        \"dtype\": \"float16\",\n        \"device\": str(device)\n    },\n    notes=\"Generation speed evaluation\",\n    run_id=run_id  # Use same run_id to group related metrics\n)\n\n# Print summary\nprint(\"\\nEvaluation Summary:\")\nprint(tracker.get_metrics(run_id=run_id))","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}