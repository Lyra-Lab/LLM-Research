{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10744015,"sourceType":"datasetVersion","datasetId":6662849}],"dockerImageVersionId":30886,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom datasets import load_dataset\nfrom datetime import datetime\nfrom typing import Dict, List, Optional, Union\nfrom tqdm import tqdm\nimport pandas as pd\nimport os\nimport time","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T08:37:06.034416Z","iopub.execute_input":"2025-02-16T08:37:06.034807Z","iopub.status.idle":"2025-02-16T08:37:16.358338Z","shell.execute_reply.started":"2025-02-16T08:37:06.034772Z","shell.execute_reply":"2025-02-16T08:37:16.357449Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# Evaluate base model on MMLU VS MMLU_RU performance","metadata":{}},{"cell_type":"markdown","source":"## This class is used to properly manager performance results","metadata":{}},{"cell_type":"code","source":"class ModelPerformanceTracker:\n    \"\"\"\n    Handles tracking and managing model performance metrics for various evaluations.\n    \"\"\"\n    def __init__(self, dataset_path=\"model_performance_metrics\"):\n        self.dataset_path = dataset_path\n        self.metrics_file = os.path.join(dataset_path, \"metrics.csv\")\n        self.ensure_dataset_structure()\n        \n    def ensure_dataset_structure(self):\n        \"\"\"Creates necessary directories and files if they don't exist.\"\"\"\n        os.makedirs(self.dataset_path, exist_ok=True)\n        if not os.path.exists(self.metrics_file):\n            df = pd.DataFrame(columns=[\n                'model_name', 'metric_type', 'metric_name', 'value',\n                'task', 'language', 'model_params', 'timestamp', \n                'run_id', 'training_config', 'notes'\n            ])\n            df.to_csv(self.metrics_file, index=False)\n\n    def log_metrics(self, \n                   metrics: Dict[str, Union[float, Dict[str, float]]], \n                   model_name: str,\n                   metric_type: str,\n                   task: Optional[str] = None,\n                   language: Optional[str] = None,\n                   model_params: Optional[dict] = None,\n                   training_config: Optional[dict] = None,\n                   notes: Optional[str] = None) -> str:\n        \"\"\"\n        Logs various model metrics to the dataset.\n        \n        Args:\n            metrics: Dictionary of metric_name:value pairs or metric_name:dict pairs\n            model_name: Name of the model being evaluated\n            metric_type: Type of metric (e.g., 'accuracy', 'speed', 'memory')\n            task: Task name if applicable\n            language: Language of evaluation if applicable\n            model_params: Model parameters/configuration\n            training_config: Training configuration if model was fine-tuned\n            notes: Additional notes about the evaluation\n        \n        Returns:\n            run_id: Unique identifier for this evaluation run\n        \"\"\"\n        df = pd.read_csv(self.metrics_file)\n        run_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        \n        new_entries = []\n        for metric_name, value in metrics.items():\n            if isinstance(value, dict):\n                # Handle nested metrics (e.g., per-task accuracies)\n                for sub_name, sub_value in value.items():\n                    entry = {\n                        'model_name': model_name,\n                        'metric_type': metric_type,\n                        'metric_name': f\"{metric_name}/{sub_name}\",\n                        'value': sub_value,\n                        'task': task,\n                        'language': language,\n                        'model_params': str(model_params) if model_params else None,\n                        'timestamp': datetime.now().isoformat(),\n                        'run_id': run_id,\n                        'training_config': str(training_config) if training_config else None,\n                        'notes': notes\n                    }\n                    new_entries.append(entry)\n            else:\n                entry = {\n                    'model_name': model_name,\n                    'metric_type': metric_type,\n                    'metric_name': metric_name,\n                    'value': value,\n                    'task': task,\n                    'language': language,\n                    'model_params': str(model_params) if model_params else None,\n                    'timestamp': datetime.now().isoformat(),\n                    'run_id': run_id,\n                    'training_config': str(training_config) if training_config else None,\n                    'notes': notes\n                }\n                new_entries.append(entry)\n        \n        df_new = pd.DataFrame(new_entries)\n        df = pd.concat([df, df_new], ignore_index=True)\n        df.to_csv(self.metrics_file, index=False)\n        \n        return run_id\n\n    def get_metrics(self, \n                   model_name: Optional[str] = None,\n                   metric_type: Optional[str] = None,\n                   task: Optional[str] = None,\n                   run_id: Optional[str] = None) -> pd.DataFrame:\n        \"\"\"\n        Retrieves metrics based on specified filters.\n        \"\"\"\n        df = pd.read_csv(self.metrics_file)\n        \n        if model_name:\n            df = df[df['model_name'] == model_name]\n        if metric_type:\n            df = df[df['metric_type'] == metric_type]\n        if task:\n            df = df[df['task'] == task]\n        if run_id:\n            df = df[df['run_id'] == run_id]\n            \n        return df\n\n    def compare_models(self, \n                      model_names: List[str],\n                      metric_type: Optional[str] = None) -> pd.DataFrame:\n        \"\"\"\n        Compares metrics between different models.\n        \"\"\"\n        df = pd.read_csv(self.metrics_file)\n        df = df[df['model_name'].isin(model_names)]\n        \n        if metric_type:\n            df = df[df['metric_type'] == metric_type]\n            \n        comparison = df.groupby(['model_name', 'metric_type', 'metric_name']).agg({\n            'value': ['mean', 'std', 'count'],\n            'timestamp': 'max'\n        }).round(4)\n        \n        return comparison","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-02-16T08:37:23.968514Z","iopub.execute_input":"2025-02-16T08:37:23.968874Z","iopub.status.idle":"2025-02-16T08:37:23.982351Z","shell.execute_reply.started":"2025-02-16T08:37:23.968822Z","shell.execute_reply":"2025-02-16T08:37:23.981348Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## These are the main evaluation function that are used to evaluate the model on any task in mmlu","metadata":{}},{"cell_type":"code","source":"def construct_prompt(example, lang=\"en\"):\n    \"\"\"\n    Constructs a prompt from a dataset example.\n    Uses language-specific keys if available and falls back to generic keys.\n    \"\"\"\n    if lang.lower() == \"ru\":\n        question_key = \"question_ru\"\n        choices_key = \"choices_ru\"\n    else:\n        question_key = \"question_en\"\n        choices_key = \"choices_en\"\n        \n    if choices_key not in example:\n        for key in [\"choices\", \"options\", \"possible_answers\", \"answers\"]:\n            if key in example:\n                choices_key = key\n                break\n    if choices_key not in example:\n        raise KeyError(\"Example must contain a valid choices key.\")\n        \n    question_text = example.get(question_key, example.get(\"question\", \"\"))\n    prompt = f\"Question: {question_text}\\nChoices:\\n\"\n    for idx, choice in enumerate(example[choices_key]):\n        letter = chr(65 + idx)  # Map 0->A, 1->B, etc.\n        prompt += f\"{letter}. {choice}\\n\"\n    prompt += \"Answer:\"\n    return prompt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T08:37:26.385828Z","iopub.execute_input":"2025-02-16T08:37:26.386167Z","iopub.status.idle":"2025-02-16T08:37:26.392393Z","shell.execute_reply.started":"2025-02-16T08:37:26.386144Z","shell.execute_reply":"2025-02-16T08:37:26.391329Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def evaluate_dataset(dataset, model, tokenizer, device):\n    \"\"\"\n    Iterates over the dataset, generates the model output for each prompt,\n    extracts the predicted answer (first capital letter found), and compares\n    it to the ground truth to compute accuracy.\n    \"\"\"\n    correct = 0\n    total = len(dataset)\n    for example in tqdm(dataset, desc=\"Evaluating\"):\n        prompt = construct_prompt(example, lang=\"ru\")\n        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=10,\n            temperature=0.0,\n            do_sample=False\n        )\n        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        generated_text = response[len(prompt):].strip()\n        answer_pred = \"\"\n        for char in generated_text:\n            if char.upper() in [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\"]:\n                answer_pred = char.upper()\n                break\n        ground_truth = example[\"answer\"]\n        if isinstance(ground_truth, int):\n            ground_truth = chr(65 + ground_truth)\n        else:\n            ground_truth = ground_truth.strip().upper()\n        if answer_pred == ground_truth:\n            correct += 1\n    accuracy = correct / total * 100\n    return accuracy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T08:37:29.544740Z","iopub.execute_input":"2025-02-16T08:37:29.545153Z","iopub.status.idle":"2025-02-16T08:37:29.552420Z","shell.execute_reply.started":"2025-02-16T08:37:29.545122Z","shell.execute_reply":"2025-02-16T08:37:29.551285Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## Initializing the model","metadata":{}},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nmodel_name = \"Qwen/Qwen2.5-7B-Instruct\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=torch.float16,\n    device_map=\"auto\"\n)\nmodel.eval()\n\ntracker = ModelPerformanceTracker()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T08:37:31.843538Z","iopub.execute_input":"2025-02-16T08:37:31.843882Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/7.30k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7e2fc6a53db44479b034c1b4b4cee6a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ead3d65e69fc4aeb90c33169b4b5e711"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3000fd9df374185bc980bc521f4abe2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39c652b7157e44b8808cbc2f5cfcd54e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/663 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d3167a0310740e0813493d172a8e472"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/27.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b78bb993d3884154a8690e13ebcdf6e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb2748a8e0604432803680e6fac35e33"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/3.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ab19e068891406e952e3c052713e802"}},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"mmlu_tasks = [\n  'abstract_algebra', 'anatomy', 'astronomy', 'auxiliary_train',\n  'business_ethics', 'clinical_knowledge', 'college_biology', 'college_chemistry',\n  'college_computer_science', 'college_mathematics', 'college_medicine', 'college_physics',\n  'computer_security', 'conceptual_physics', 'econometrics', 'electrical_engineering',\n  'elementary_mathematics', 'formal_logic', 'global_facts', 'high_school_biology',\n  'high_school_chemistry', 'high_school_computer_science', 'high_school_european_history',\n  'high_school_geography', 'high_school_government_and_politics', 'high_school_macroeconomics',\n  'high_school_mathematics', 'high_school_microeconomics', 'high_school_physics',\n  'high_school_psychology', 'high_school_statistics', 'high_school_us_history',\n  'high_school_world_history', 'human_aging', 'human_sexuality', 'international_law',\n  'jurisprudence', 'logical_fallacies', 'machine_learning', 'management', 'marketing',\n  'medical_genetics', 'miscellaneous', 'moral_disputes', 'moral_scenarios', 'nutrition',\n  'philosophy', 'prehistory', 'professional_accounting', 'professional_law',\n  'professional_medicine', 'professional_psychology', 'public_relations', 'security_studies',\n  'sociology', 'us_foreign_policy', 'virology', 'world_religions'\n]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Evaluation","metadata":{}},{"cell_type":"code","source":"# Evaluate MMLU_RU performance\nmmlu_results = {}\nfor task in mmlu_tasks:\n    try:\n        dataset_mmlu = load_dataset(\"NLPCoreTeam/mmlu_ru\", task, split=\"test\")\n        accuracy = evaluate_dataset(dataset_mmlu, model, tokenizer, device)\n        mmlu_results[task] = accuracy\n    except Exception as e:\n        print(f\"Error evaluating {task}: {e}\")\n\n# Log MMLU_RU results\nrun_id = tracker.log_metrics(\n    metrics={'task_accuracy': mmlu_results},\n    model_name=model_name,\n    metric_type='accuracy',\n    language=\"ru\",\n    model_params={\n        \"model_size\": \"7B\",\n        \"dtype\": \"float16\",\n        \"device\": str(device)\n    },\n    notes=\"MMLU-RU evaluation\"\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Evaluate MMLU performance\nmmlu_results = {}\nfor task in mmlu_tasks:\n    try:\n        # Load the original MMLU dataset\n        dataset_mmlu = load_dataset(\"cais/mmlu\", task, split=\"test\")\n        # Evaluate the dataset\n        accuracy = evaluate_dataset(dataset_mmlu, model, tokenizer, device)\n        # Store the results\n        mmlu_results[task] = accuracy\n    except Exception as e:\n        print(f\"Error evaluating {task}: {e}\")\n\n# Log MMLU results\nrun_id = tracker.log_metrics(\n    metrics={'task_accuracy': mmlu_results},\n    model_name=model_name,\n    metric_type='accuracy',\n    language=\"en\",\n    model_params={\n        \"model_size\": \"7B\",\n        \"dtype\": \"float16\",\n        \"device\": str(device)\n    },\n    notes=\"MMLU evaluation\"\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Evaluate the speed of text generation in Russian VS English","metadata":{}},{"cell_type":"code","source":"def evaluate_generation_speed(model, tokenizer, device, \n                            num_samples=100, \n                            input_lengths=[128, 512], \n                            output_lengths=[128, 512]):\n    \"\"\"\n    Evaluates model generation speed for different input and output lengths.\n    \n    Returns:\n        Dictionary containing various speed metrics:\n        - tokens_per_second: Generation speed\n        - latency_ms: Average latency per generation\n        - throughput: Tokens processed per second including input\n    \"\"\"\n    metrics = {}\n    \n    # Generate sample input text of different lengths\n    sample_text = \"The quick brown fox jumps over the lazy dog. \" * 100\n    \n    for input_len in input_lengths:\n        for output_len in output_lengths:\n            total_generation_time = 0\n            total_tokens = 0\n            \n            # Truncate input text to desired length\n            input_text = sample_text[:input_len]\n            \n            # Warm-up run\n            inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n            _ = model.generate(**inputs, max_new_tokens=output_len)\n            \n            # Timed runs\n            for _ in range(num_samples):\n                inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n                start_time = time.time()\n                outputs = model.generate(**inputs, max_new_tokens=output_len)\n                end_time = time.time()\n                \n                generation_time = end_time - start_time\n                total_generation_time += generation_time\n                total_tokens += len(outputs[0]) - len(inputs['input_ids'][0])\n            \n            # Calculate metrics\n            avg_latency = (total_generation_time / num_samples) * 1000  # ms\n            tokens_per_second = total_tokens / total_generation_time\n            \n            metrics[f\"input_{input_len}_output_{output_len}\"] = {\n                'latency_ms': avg_latency,\n                'tokens_per_second': tokens_per_second,\n            }\n    \n    return metrics","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Evaluate generation speed\nspeed_metrics = evaluate_generation_speed(\n    model, \n    tokenizer, \n    device,\n    num_samples=10  # Reduced for testing, increase for better statistics\n)\n\n# Log speed metrics\ntracker.log_metrics(\n    metrics=speed_metrics,\n    model_name=model_name,\n    metric_type='speed',\n    model_params={\n        \"model_size\": \"7B\",\n        \"dtype\": \"float16\",\n        \"device\": str(device)\n    },\n    notes=\"Generation speed evaluation\",\n    run_id=run_id  # Use same run_id to group related metrics\n)\n\n# Print summary\nprint(\"\\nEvaluation Summary:\")\nprint(tracker.get_metrics(run_id=run_id))","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}